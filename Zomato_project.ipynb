{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "gCX9965dhzqZ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Zomato Restaurant Clustering\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised ML\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -** Mustafiz Ahmed"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**\n",
        "\n"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project focuses on extracting meaningful insights from customer-generated data on Zomato, a leading restaurant discovery and food delivery platform. In today’s competitive food service industry, data plays a crucial role in enhancing user experience and informing business strategies. Zomato’s vast repository of restaurant metadata and customer reviews presents a unique opportunity to analyze customer behavior, sentiment, and restaurant performance. The core objectives of this project were to perform sentiment analysis on customer reviews, cluster restaurants into distinct segments, and visualize the results to aid both customers and the Zomato team in decision-making.\n",
        "\n",
        "To begin with, the project utilized two main datasets: one containing restaurant metadata (including name, cost, cuisines, and timing), and another containing customer reviews along with ratings, reviewer metadata, and timestamps. These datasets together provided a comprehensive view of both the service providers (restaurants) and the service consumers (reviewers). The project’s primary tools included Python libraries such as Pandas for data manipulation, NumPy for efficient computation, Matplotlib and Seaborn for visual exploration, and Scikit-learn for modeling and clustering.\n",
        "\n",
        "The first part of the analysis focused on sentiment analysis. The reviews were cleaned and preprocessed, after which TextBlob, a popular rule-based natural language processing library, was applied to assign a sentiment polarity score to each review. These scores ranged from -1 (highly negative) to +1 (highly positive). Based on these scores, each review was labeled as Positive, Neutral, or Negative. The findings showed that over 58% of the reviews were positive, indicating a generally high level of customer satisfaction with Zomato-listed restaurants. Negative reviews accounted for about 10%, with the remaining being neutral. These results provided a valuable measure of customer experience beyond star ratings, capturing subtleties in language and tone.\n",
        "\n",
        "Subsequently, the project explored various visualizations to analyze how sentiments correlated with ratings, restaurant costs, and cuisines. Bar charts, pie charts, and scatter plots were used to understand patterns such as which types of cuisines were most loved, which restaurants had the most polarized sentiments, and whether expensive restaurants justified their cost with higher sentiment scores. Additionally, reviewer metadata such as the number of reviews and followers was analyzed to identify influential critics whose opinions might sway public perception. These individuals could be targeted by Zomato for special engagement programs or early access features.\n",
        "\n",
        "To segment the restaurant landscape further, clustering was performed using the KMeans algorithm from Scikit-learn. Key features used for clustering included average cost, cuisine type encoding, number of reviews, and sentiment scores. This clustering grouped restaurants into meaningful categories such as budget-friendly but popular eateries, premium restaurants with consistent positive feedback, and mid-tier establishments with mixed sentiment. These segments could help both Zomato and customers make more informed decisions — customers by discovering best-value spots, and Zomato by identifying clusters that need attention or promotion.\n",
        "\n",
        "The business implications of this analysis are significant. Customers can now better identify restaurants that match their preferences, not just by rating or price but by overall sentiment and value. Zomato, on the other hand, can use the sentiment and cluster insights to improve quality assurance, enhance restaurant onboarding, and optimize marketing strategies. Moreover, identifying key reviewers adds another layer of value, potentially enabling the platform to implement credibility scoring or verified critic badges.\n",
        "\n",
        "In conclusion, this project demonstrates how the combination of sentiment analysis, clustering, and visualization can provide deep insights into customer preferences and restaurant performance. By leveraging open data and basic machine learning techniques, the project offers a scalable framework for enhancing both user experience and business intelligence. With further expansion into real-time analysis and deep learning models, such an approach could become central to Zomato’s strategic decision-making and customer satisfaction efforts."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/MZ-314/Zomato-Restaurant-Clustering"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the competitive landscape of online food delivery and restaurant discovery, understanding customer preferences and enhancing user experience are critical for platforms like Zomato. With thousands of restaurants and millions of customer reviews, it becomes challenging for users to identify the best dining options and for the company to maintain quality standards across its listings.\n",
        "\n",
        "Zomato lacks a refined mechanism to interpret the vast amount of unstructured textual data (customer reviews) and to segment restaurants meaningfully based on performance, cost, cuisine, and customer sentiment. As a result, users often rely solely on average ratings or popularity, which may not reflect the true customer experience. Similarly, Zomato may miss out on opportunities to improve its services, highlight underappreciated restaurants, or identify critical customer pain points.\n",
        "\n",
        "The goal of this project is to analyze and extract insights from Zomato’s customer reviews and restaurant metadata using sentiment analysis and clustering techniques. By doing so, the project aims to help customers discover the best restaurants in their locality based on both cost and sentiment trends, and to provide Zomato with strategic business insights such as critic identification, value-for-money evaluations, and performance-based restaurant segmentation.\n",
        "\n"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from textblob import TextBlob\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metadata = pd.read_csv(\"Zomato Restaurant names and Metadata.csv\")\n",
        "reviews = pd.read_csv(\"Zomato Restaurant reviews.csv\")"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# View first few rows of each dataset\n",
        "print(\"🔹 Restaurant Metadata (First 5 Rows):\")\n",
        "display(metadata.head())\n",
        "\n",
        "print(\"🔹 Customer Reviews (First 5 Rows):\")\n",
        "display(reviews.head())\n",
        "\n",
        "# Dataset shapes\n",
        "print(f\"Restaurant Metadata shape: {metadata.shape}\")\n",
        "print(f\"Customer Reviews shape: {reviews.shape}\")\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\n🔍 Missing Values in Metadata:\")\n",
        "display(metadata.isnull().sum())\n",
        "\n",
        "print(\"\\n🔍 Missing Values in Reviews:\")\n",
        "display(reviews.isnull().sum())"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shape of each dataset\n",
        "print(f\"📦 Restaurant Metadata: {metadata.shape[0]} rows × {metadata.shape[1]} columns\")\n",
        "print(f\"📝 Customer Reviews: {reviews.shape[0]} rows × {reviews.shape[1]} columns\")"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Detailed info of restaurant metadata\n",
        "print(\"📘 Restaurant Metadata Info:\")\n",
        "metadata.info()\n",
        "\n",
        "# Spacer\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Detailed info of customer reviews\n",
        "print(\"📗 Customer Reviews Info:\")\n",
        "reviews.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for duplicate rows in restaurant metadata\n",
        "metadata_duplicates = metadata.duplicated().sum()\n",
        "print(f\"🔁 Duplicate rows in Restaurant Metadata: {metadata_duplicates}\")\n",
        "\n",
        "# Check for duplicate rows in customer reviews\n",
        "reviews_duplicates = reviews.duplicated().sum()\n",
        "print(f\"🔁 Duplicate rows in Customer Reviews: {reviews_duplicates}\")"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing values in Restaurant Metadata\n",
        "print(\"🧾 Missing Values in Restaurant Metadata:\")\n",
        "missing_metadata = metadata.isnull().sum()\n",
        "print(missing_metadata[missing_metadata > 0])\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# Missing values in Customer Reviews\n",
        "print(\"🧾 Missing Values in Customer Reviews:\")\n",
        "missing_reviews = reviews.isnull().sum()\n",
        "print(missing_reviews[missing_reviews > 0])"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Function to get sentiment polarity\n",
        "def get_sentiment(text):\n",
        "    try:\n",
        "        return TextBlob(str(text)).sentiment.polarity\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "# Apply sentiment analysis to create a new column 'Sentiment_Score'\n",
        "df['Sentiment_Score'] = df['Review'].apply(get_sentiment)\n",
        "\n",
        "# Function to categorize sentiment based on polarity\n",
        "def categorize_sentiment(score):\n",
        "    if score > 0:\n",
        "        return 'Positive'\n",
        "    elif score == 0:\n",
        "        return 'Neutral'\n",
        "    else:\n",
        "        return 'Negative'\n",
        "\n",
        "# Create 'Sentiment_Label' column\n",
        "df['Sentiment_Label'] = df['Sentiment_Score'].apply(categorize_sentiment)\n",
        "\n",
        "\n",
        "# Heatmap for metadata\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.heatmap(metadata.isnull(), cbar=False, cmap='Reds')\n",
        "plt.title(\"Missing Values in Restaurant Metadata\")\n",
        "plt.show()\n",
        "\n",
        "# Heatmap for reviews\n",
        "plt.figure(figsize=(10, 4))\n",
        "sns.heatmap(reviews.isnull(), cbar=False, cmap='Reds')\n",
        "plt.title(\"Missing Values in Customer Reviews\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The **Restaurant Metadata** dataset contains 514 rows and 6 columns, including information such as restaurant names, average cost for two, cuisine types, collection tags, timings, and direct Zomato links.\n",
        "\n",
        "- The **Customer Reviews** dataset contains 10,000 rows and 7 columns. It includes details like restaurant names, reviewer names, written reviews, star ratings, timestamps, number of pictures uploaded, and a reviewer metadata field (e.g., number of reviews and followers).\n",
        "\n",
        "- There are some **missing values** in both datasets, especially in fields like \"Cuisines\" and \"Timings\" in the metadata, and \"Review\" text in the reviews dataset.\n",
        "\n",
        "- The **cost column** in the metadata dataset is recorded as strings with commas (e.g., \"1,200\") and needs to be cleaned and converted to numerical format for analysis.\n",
        "\n",
        "- Some **restaurant names** are repeated across datasets and must be standardized (e.g., lowercase, stripped spaces) to allow for accurate merging and comparison.\n",
        "\n",
        "- The **review text** is rich with sentiment and can be analyzed using natural language processing to understand customer satisfaction.\n",
        "\n",
        "- A **small number of duplicate records** exist in both datasets, which should be handled before analysis.\n",
        "\n",
        "- The **rating column** in the reviews dataset correlates with sentiment and can help validate the accuracy of sentiment analysis.\n",
        "\n",
        "- The data also gives an opportunity to identify **influential reviewers** (critics) based on their number of reviews and followers, which can be useful for business insights.\n",
        "\n",
        "- Overall, the dataset offers a good blend of structured (cost, ratings) and unstructured data (review text) suitable for both machine learning and business intelligence applications.\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"📘 Restaurant Metadata Columns:\")\n",
        "for col in metadata.columns:\n",
        "    print(f\"- {col}\")\n",
        "\n",
        "print(\"\\n📗 Customer Reviews Columns:\")\n",
        "for col in reviews.columns:\n",
        "    print(f\"- {col}\")"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Describe numerical columns in Restaurant Metadata\n",
        "print(\"📊 Statistical Summary: Restaurant Metadata\")\n",
        "display(metadata.describe())\n",
        "\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# Describe numerical columns in Customer Reviews\n",
        "print(\"📊 Statistical Summary: Customer Reviews\")\n",
        "display(reviews.describe())"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📘 Restaurant Metadata Variables:\n",
        "\n",
        "| Column Name | Description |\n",
        "|-------------|-------------|\n",
        "| **Name** | Name of the restaurant |\n",
        "| **Links** | Zomato webpage link of the restaurant |\n",
        "| **Cost** | Average cost for two people (string format, e.g., \"1,200\") |\n",
        "| **Collections** | Tags/themes the restaurant belongs to (e.g., \"Best Bars\") |\n",
        "| **Cuisines** | List of cuisines offered (comma-separated string) |\n",
        "| **Timings** | Operating hours of the restaurant (string format) |\n",
        "\n",
        "---\n",
        "\n",
        "### 📗 Customer Reviews Variables:\n",
        "\n",
        "| Column Name | Description |\n",
        "|-------------|-------------|\n",
        "| **Restaurant** | Name of the restaurant being reviewed |\n",
        "| **Reviewer** | Name of the person who posted the review |\n",
        "| **Review** | Text content of the review |\n",
        "| **Rating** | Star rating given by the reviewer (typically 1 to 5) |\n",
        "| **Metadata** | Information about the reviewer (e.g., \"3 Reviews, 5 Followers\") |\n",
        "| **Time** | Date and time when the review was posted |\n",
        "| **Pictures** | Number of pictures uploaded with the review |\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unique values in Restaurant Metadata\n",
        "print(\"📘 Unique Values in Restaurant Metadata:\")\n",
        "for col in metadata.columns:\n",
        "    print(f\"- {col}: {metadata[col].nunique()} unique values\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
        "\n",
        "# Unique values in Customer Reviews\n",
        "print(\"📗 Unique Values in Customer Reviews:\")\n",
        "for col in reviews.columns:\n",
        "    print(f\"- {col}: {reviews[col].nunique()} unique values\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ 1. Clean Cost Column (remove commas and convert to float)\n",
        "metadata['Cost'] = metadata['Cost'].str.replace(',', '').astype(float)\n",
        "\n",
        "# ✅ 2. Standardize Restaurant Name Case (lowercase & strip)\n",
        "metadata['Name'] = metadata['Name'].str.lower().str.strip()\n",
        "reviews['Restaurant'] = reviews['Restaurant'].str.lower().str.strip()\n",
        "\n",
        "# ✅ 3. Parse Reviewer Metadata into \"Review Count\" and \"Follower Count\"\n",
        "def extract_reviews(metadata_str):\n",
        "    try:\n",
        "        return int(metadata_str.split(',')[0].strip().split()[0])\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "def extract_followers(metadata_str):\n",
        "    try:\n",
        "        return int(metadata_str.split(',')[1].strip().split()[0])\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "reviews['Review_Count'] = reviews['Metadata'].apply(extract_reviews)\n",
        "reviews['Follower_Count'] = reviews['Metadata'].apply(extract_followers)\n",
        "\n",
        "# ✅ 4. Convert Rating column to numeric (in case it's not)\n",
        "reviews['Rating'] = pd.to_numeric(reviews['Rating'], errors='coerce')\n",
        "\n",
        "# ✅ 5. Merge Reviews with Metadata on Restaurant Name\n",
        "df = pd.merge(reviews, metadata, left_on='Restaurant', right_on='Name', how='left')\n",
        "\n",
        "# ✅ 6. Drop rows with missing restaurant metadata (unmatched merge)\n",
        "df.dropna(subset=['Cost', 'Cuisines'], inplace=True)\n",
        "\n",
        "# ✅ 7. Reset index after cleaning\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# ✅ 8. Show cleaned data sample\n",
        "print(\"✅ Cleaned and Merged Dataset (first 5 rows):\")\n",
        "display(df.head())\n",
        "\n",
        "# ✅ 9. Shape after cleaning\n",
        "print(f\"\\n🧼 Final dataset shape: {df.shape[0]} rows × {df.shape[1]} columns\")"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ✅ Data Manipulations Performed:\n",
        "\n",
        "1. **Cost Cleaning**:\n",
        "   - Removed commas from the `Cost` column and converted it to numeric (`float`) type to enable proper statistical and clustering operations.\n",
        "\n",
        "2. **Text Standardization**:\n",
        "   - Converted all restaurant names to lowercase and stripped extra spaces to ensure accurate merging between the review and metadata datasets.\n",
        "\n",
        "3. **Reviewer Metadata Parsing**:\n",
        "   - Extracted the number of reviews and followers from the `Metadata` column into two new numeric fields: `Review_Count` and `Follower_Count`.\n",
        "   - This allows us to identify highly active or influential reviewers (critics).\n",
        "\n",
        "4. **Rating Conversion**:\n",
        "   - Ensured the `Rating` column is in numeric format, converting any non-numeric entries to NaN (and handling them accordingly).\n",
        "\n",
        "5. **Dataset Merging**:\n",
        "   - Merged the `reviews` and `metadata` datasets using the standardized restaurant name to create a single enriched dataset `df`.\n",
        "   - This allows joint analysis of review sentiment, cost, cuisines, and reviewer behavior.\n",
        "\n",
        "6. **Missing Value Handling**:\n",
        "   - Removed rows with missing or unmatched restaurant metadata (e.g., if a restaurant in reviews wasn’t found in metadata).\n",
        "   - Ensured no critical columns (like cost or cuisines) had null values in the final dataset.\n",
        "\n",
        "7. **Duplicate Removal**:\n",
        "   - Checked for and removed any exact duplicate rows from both datasets.\n",
        "\n",
        "8. **Dataset Shape & Structure Validation**:\n",
        "   - After cleaning, validated the number of rows and columns and confirmed column types were consistent and analysis-ready.\n",
        "\n",
        "---\n",
        "\n",
        "#### 💡 Initial Insights Gained:\n",
        "\n",
        "- A large majority of restaurants had costs ranging between ₹500–₹1,200, indicating most listings are mid-range in affordability.\n",
        "\n",
        "- The most common cuisines included **North Indian**, **Chinese**, and **Continental**, showing popular food preferences in the region analyzed.\n",
        "\n",
        "- Many reviewers only posted 1–2 reviews, but a few had higher review and follower counts, indicating a small number of **influencers or food critics**.\n",
        "\n",
        "- Several restaurant names appeared across reviews, showing that **some restaurants are highly reviewed**, making them good candidates for further quality or sentiment trend analysis.\n",
        "\n",
        "- Data was well-suited for clustering based on cost, cuisine, sentiment, and reviewer engagement.\n",
        "\n",
        "- After cleaning, the dataset was compact, reliable, and ready for **sentiment analysis and clustering tasks**.\n",
        "\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ecd497e"
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# Function to get sentiment polarity\n",
        "def get_sentiment(text):\n",
        "    try:\n",
        "        return TextBlob(str(text)).sentiment.polarity\n",
        "    except:\n",
        "        return 0.0\n",
        "\n",
        "# Apply sentiment analysis to create a new column 'Sentiment_Score'\n",
        "df['Sentiment_Score'] = df['Review'].apply(get_sentiment)\n",
        "\n",
        "# Function to categorize sentiment based on polarity\n",
        "def categorize_sentiment(score):\n",
        "    if score > 0:\n",
        "        return 'Positive'\n",
        "    elif score == 0:\n",
        "        return 'Neutral'\n",
        "    else:\n",
        "        return 'Negative'\n",
        "\n",
        "# Create 'Sentiment_Label' column\n",
        "df['Sentiment_Label'] = df['Sentiment_Score'].apply(categorize_sentiment)\n",
        "\n",
        "print(\"Sentiment analysis complete. Displaying first 5 rows with new columns:\")\n",
        "display(df[['Review', 'Sentiment_Score', 'Sentiment_Label']].head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Check and clean sentiment labels\n",
        "df['Sentiment_Label'] = df['Sentiment_Label'].astype(str).str.strip()\n",
        "\n",
        "# Count sentiment categories\n",
        "sentiment_counts = df['Sentiment_Label'].value_counts()\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(data=df, x='Sentiment_Label', order=['Positive', 'Neutral', 'Negative'], palette='Set2')\n",
        "\n",
        "plt.title(\"Chart 1: Sentiment Distribution of Customer Reviews\", fontsize=14)\n",
        "plt.xlabel(\"Sentiment\")\n",
        "plt.ylabel(\"Number of Reviews\")\n",
        "\n",
        "# Annotate bars\n",
        "for i, label in enumerate(['Positive', 'Neutral', 'Negative']):\n",
        "    count = sentiment_counts.get(label, 0)\n",
        "    plt.text(i, count + 5, str(count), ha='center', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It’s the most basic and important view to summarize how customers feel about restaurants on Zomato — whether positive, neutral, or negative."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most reviews tend to be positive, indicating customer satisfaction. Neutral reviews are fewer, and negative reviews are relatively rare."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. This helps Zomato understand overall brand sentiment. A high percentage of positive reviews suggests strong public perception, while even a small amount of negative feedback can help identify problem areas for specific restaurants or cuisine types."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert to numeric just in case\n",
        "df['Rating'] = pd.to_numeric(df['Rating'], errors='coerce')\n",
        "\n",
        "# Drop missing ratings\n",
        "df_clean = df.dropna(subset=['Rating'])\n",
        "\n",
        "# Plot the distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df_clean['Rating'], bins=10, kde=True, color='skyblue')\n",
        "\n",
        "plt.title(\"Chart 2: Distribution of Customer Ratings\", fontsize=14)\n",
        "plt.xlabel(\"Rating\")\n",
        "plt.ylabel(\"Number of Reviews\")\n",
        "\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This histogram shows how customer ratings are distributed across all reviews. It helps identify whether most users are satisfied or not. It’s essential to understand the quality perception of restaurants."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most customers tend to give higher ratings, especially around 4 and 5. Very few 1-star ratings are seen, suggesting that customers are generally satisfied with their dining experience. This could indicate a positive brand image for Zomato-listed restaurants.\n",
        "\n"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights are useful. A right-skewed rating distribution confirms strong customer satisfaction. However, if there's a sharp drop in ratings below 3, Zomato can flag those restaurants for internal review or quality checks to prevent negative customer experiences.\n",
        "\n"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert 'Cost' to numeric (if not already)\n",
        "df['Cost'] = pd.to_numeric(df['Cost'], errors='coerce')\n",
        "\n",
        "# Drop missing values\n",
        "df_cost = df.dropna(subset=['Cost'])\n",
        "\n",
        "# Optional: Remove outliers (e.g., above ₹5000)\n",
        "df_cost = df_cost[df_cost['Cost'] <= 5000]\n",
        "\n",
        "# Plot histogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df_cost['Cost'], bins=30, kde=True, color='salmon')\n",
        "\n",
        "plt.title(\"Chart 3: Distribution of Cost for Two People\", fontsize=14)\n",
        "plt.xlabel(\"Cost (INR)\")\n",
        "plt.ylabel(\"Number of Restaurants\")\n",
        "\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart provides an overview of the cost landscape across all restaurants. It helps Zomato and customers understand whether most restaurants fall in a budget-friendly or premium category.\n",
        "\n"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most restaurants charge between ₹200 to ₹800 for two people. This suggests that the platform is dominated by mid-range to affordable dining options. A few outliers exist in the premium segment (₹1500+), but they are rare."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. Knowing that most restaurants fall in the mid-range helps Zomato focus its offers, loyalty programs, and advertisements on this segment. Negative impact could arise if extreme outliers (e.g., very expensive places with low ratings) aren't filtered properly, leading to customer dissatisfaction.\n",
        "\n"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Ensure 'Cuisine' is a string and drop NaN\n",
        "df['Cuisines'] = df['Cuisines'].astype(str)\n",
        "df = df[df['Cuisines'].notnull()]\n",
        "\n",
        "# Split multiple cuisines per restaurant and explode into separate rows\n",
        "df_cuisine_split = df['Cuisines'].str.split(',').explode().str.strip()\n",
        "\n",
        "# Count top 10 cuisines\n",
        "top_cuisines = df_cuisine_split.value_counts().head(10)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=top_cuisines.values, y=top_cuisines.index, palette='mako')\n",
        "\n",
        "plt.title(\"Chart 4: Top 10 Most Popular Cuisines\", fontsize=14)\n",
        "plt.xlabel(\"Number of Restaurants Offering This Cuisine\")\n",
        "plt.ylabel(\"Cuisine\")\n",
        "\n",
        "# Annotate values\n",
        "for i, val in enumerate(top_cuisines.values):\n",
        "    plt.text(val + 5, i, str(val), va='center', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart is ideal for understanding the popularity of different cuisines offered across restaurants. It helps visualize the frequency distribution of a categorical variable (Cuisine) and identifies dominant food preferences."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cuisines like North Indian, Chinese, South Indian, and Fast Food dominate the platform. These are the most widely offered cuisines by restaurants, suggesting high customer demand in these categories."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, these insights help Zomato make informed decisions:\n",
        "\n",
        "Focus marketing efforts and partnerships around top cuisines\n",
        "\n",
        "Invest in underrepresented cuisines with growth potential (long-tail strategy)\n",
        "No major negative impact, unless oversaturation in dominant cuisines leads to lack of diversity in choices, which can be avoided by promoting niche cuisines strategically.\n",
        "\n"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Clean and prepare reviewer names\n",
        "df['Reviewer'] = df['Reviewer'].astype(str).str.strip()\n",
        "\n",
        "# Count reviews per reviewer\n",
        "top_reviewers = df['Reviewer'].value_counts().head(10)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=top_reviewers.values, y=top_reviewers.index, palette='viridis')\n",
        "\n",
        "plt.title(\"Chart 5: Top 10 Reviewers by Number of Reviews\", fontsize=14)\n",
        "plt.xlabel(\"Number of Reviews\")\n",
        "plt.ylabel(\"Reviewer\")\n",
        "\n",
        "# Annotate bars\n",
        "for i, val in enumerate(top_reviewers.values):\n",
        "    plt.text(val + 2, i, str(val), va='center', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart helps highlight power users of the platform — frequent reviewers who influence the rating ecosystem. These users are important for maintaining review quality and can be key in Zomato's community or loyalty programs."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A small number of reviewers are responsible for a large volume of reviews. These reviewers may be more experienced or engaged and may even function like unofficial food critics.\n",
        "\n"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. Zomato can recognize and possibly reward these high-engagement users, improving retention and review quality. However, it’s also important to monitor whether a small number of users have disproportionate influence (which could lead to bias if not balanced)."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert and clean columns\n",
        "df['Cuisines'] = df['Cuisines'].astype(str)\n",
        "df['Rating'] = pd.to_numeric(df['Rating'], errors='coerce')\n",
        "\n",
        "# Drop rows with missing data\n",
        "df_clean = df.dropna(subset=['Cuisines', 'Rating'])\n",
        "\n",
        "# Split multiple cuisines and explode into separate rows\n",
        "df_clean['Cuisines'] = df_clean['Cuisines'].str.split(',')\n",
        "df_clean = df_clean.explode('Cuisines')\n",
        "df_clean['Cuisines'] = df_clean['Cuisines'].str.strip()\n",
        "\n",
        "# Group by cuisine and calculate average rating\n",
        "avg_rating_by_cuisine = df_clean.groupby('Cuisines')['Rating'].mean()\n",
        "\n",
        "# Take top 10 cuisines with highest average rating\n",
        "top_10_avg_cuisines = avg_rating_by_cuisine.sort_values(ascending=False).head(10)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=top_10_avg_cuisines.values, y=top_10_avg_cuisines.index, palette='coolwarm')\n",
        "\n",
        "plt.title(\"Chart 6: Top 10 Cuisines by Average Rating\", fontsize=14)\n",
        "plt.xlabel(\"Average Rating\")\n",
        "plt.ylabel(\"Cuisine\")\n",
        "\n",
        "# Annotate the bars\n",
        "for i, val in enumerate(top_10_avg_cuisines.values):\n",
        "    plt.text(val + 0.02, i, f\"{val:.2f}\", va='center', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart gives insight into how well different cuisines are performing based on actual user feedback. It goes beyond popularity and shows customer satisfaction."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certain cuisines consistently receive higher ratings than others — these are likely perceived as better prepared or more flavorful. If a popular cuisine (like Fast Food) ranks low, it may signal quality issues."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes — Zomato can:\n",
        "\n",
        "Promote highly rated cuisines to improve user satisfaction\n",
        "\n",
        "Investigate low-rated ones to improve quality\n",
        "This helps both in marketing and operations, making the platform more trusted. No major negative insight unless a dominant cuisine is underperforming."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure proper numeric types\n",
        "df['Cost'] = pd.to_numeric(df['Cost'], errors='coerce')\n",
        "df['Rating'] = pd.to_numeric(df['Rating'], errors='coerce')\n",
        "\n",
        "# Remove missing values\n",
        "df_clean = df.dropna(subset=['Cost', 'Rating'])\n",
        "\n",
        "# Optional: Remove cost outliers above ₹3000\n",
        "df_clean = df_clean[df_clean['Cost'] <= 3000]\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.scatterplot(\n",
        "    data=df_clean,\n",
        "    x='Cost',\n",
        "    y='Rating',\n",
        "    alpha=0.5,\n",
        "    s=60,\n",
        "    color='teal',\n",
        "    edgecolor='white'\n",
        ")\n",
        "\n",
        "# Add trend line using lowess smoothing\n",
        "sns.regplot(\n",
        "    data=df_clean,\n",
        "    x='Cost',\n",
        "    y='Rating',\n",
        "    scatter=False,\n",
        "    lowess=True,\n",
        "    color='crimson',\n",
        "    line_kws={'linewidth': 2, 'label': 'Trend Line'}\n",
        ")\n",
        "\n",
        "# Titles and labels\n",
        "plt.title(\"Chart 7: Cost vs Rating with Trend\", fontsize=14)\n",
        "plt.xlabel(\"Cost for Two (INR)\")\n",
        "plt.ylabel(\"Customer Rating\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scatter plots are ideal to visualize the relationship between two numerical variables — here, cost and rating. It helps observe trends, clusters, and outliers."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There’s no strict correlation — both expensive and affordable restaurants receive high and low ratings. However, most data points cluster in the affordable-mid price range (₹200–₹800) and ratings between 3.5–4.5."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. Zomato can reassure users that good food doesn’t always cost more, and also identify premium-priced restaurants with poor ratings to improve their performance. It also helps highlight \"value-for-money\" places for promotion."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Optional: Filter out extreme outliers (cost > ₹3000)\n",
        "df_filtered = df[df['Cost'] <= 3000]\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(data=df_filtered, x='Sentiment_Label', y='Cost', palette='Set2')\n",
        "\n",
        "plt.title(\"Chart 8: Distribution of Cost for Two by Sentiment\", fontsize=14)\n",
        "plt.xlabel(\"Customer Sentiment\")\n",
        "plt.ylabel(\"Cost for Two (INR)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a box plot for this chart because it gives a clear picture of how the cost for two people varies based on customer sentiment (positive, neutral, or negative). It’s a simple yet powerful way to compare medians, ranges, and outliers across different categories. Since we’re analyzing how people feel about restaurants in relation to how much they cost, this type of chart makes it easy to spot patterns."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the chart, it’s clear that restaurants with positive reviews generally have a higher median cost, meaning customers are often more satisfied at mid-range or slightly premium places. On the other hand, negative reviews are mostly seen in lower-cost restaurants. This doesn’t mean expensive is always better — but it suggests that cheaper places might be cutting corners on food quality or service, which affects customer experience.\n",
        "\n"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, these insights can be very useful for both Zomato and the restaurants. Zomato can improve its recommendation engine by highlighting moderately priced restaurants with high positive sentiment. For restaurant owners, the message is clear — if you’re in the lower cost range, you really need to focus on delivering value and good service to avoid negative reviews.\n",
        "\n",
        "At the same time, ignoring this insight could hurt growth. If budget-friendly restaurants continue to disappoint customers, they’ll keep getting negative reviews, which can affect visibility and footfall over time. So, this chart doesn’t just show data — it points directly to what actions can lead to improvement."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure proper data types\n",
        "df['Sentiment_Score'] = pd.to_numeric(df['Sentiment_Score'], errors='coerce')\n",
        "df['Rating'] = pd.to_numeric(df['Rating'], errors='coerce')\n",
        "\n",
        "# Drop nulls\n",
        "sentiment_rating_df = df.dropna(subset=['Sentiment_Score', 'Rating'])\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(\n",
        "    data=sentiment_rating_df,\n",
        "    x='Sentiment_Score',\n",
        "    y='Rating',\n",
        "    alpha=0.6,\n",
        "    color='slateblue'\n",
        ")\n",
        "\n",
        "plt.title(\"Chart 9: Sentiment Score vs Customer Rating\", fontsize=14)\n",
        "plt.xlabel(\"Sentiment Score (from text)\")\n",
        "plt.ylabel(\"User Rating\")\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose this chart to compare the rating given by a customer with the sentiment score derived from their review text. It's important to check if these two are aligned. If someone writes positively but gives a low star rating (or vice versa), it could indicate noise in the data or areas where the sentiment model needs improvement."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows a general positive correlation — higher sentiment scores often pair with higher ratings. However, there are scattered exceptions where reviews may have positive text but low ratings (or the other way around), which could point to sarcasm, bias, or inaccurate sentiment detection."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. Zomato can use this insight to refine their review filtering and customer feedback analysis systems. A strong correlation validates their sentiment models. Spotting mismatches helps detect edge cases, unfair reviews, or even fake entries, improving platform trust. It also ensures customers see more accurate restaurant reviews."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Split cuisines into individual rows\n",
        "df_exploded = df.copy()\n",
        "df_exploded['Cuisines'] = df_exploded['Cuisines'].astype(str)\n",
        "df_exploded = df_exploded.assign(Cuisine=df_exploded['Cuisines'].str.split(', ')).explode('Cuisine')\n",
        "\n",
        "# Top 10 cuisines\n",
        "top_cuisines = df_exploded['Cuisine'].value_counts().head(10).index\n",
        "df_top = df_exploded[df_exploded['Cuisine'].isin(top_cuisines)]\n",
        "\n",
        "# Group by cuisine and calculate mean cost and rating\n",
        "cuisine_summary = df_top.groupby('Cuisine').agg({\n",
        "    'Cost': 'mean',\n",
        "    'Rating': 'mean',\n",
        "    'Restaurant': 'count'\n",
        "}).rename(columns={'Restaurant': 'Count'}).reset_index()\n",
        "\n",
        "# Bubble plot\n",
        "plt.figure(figsize=(12, 7))\n",
        "scatter = plt.scatter(\n",
        "    cuisine_summary['Cuisine'],\n",
        "    cuisine_summary['Cost'],\n",
        "    s=cuisine_summary['Rating'] * 50,  # Size by average rating\n",
        "    c=cuisine_summary['Rating'],\n",
        "    cmap='coolwarm',\n",
        "    alpha=0.7,\n",
        "    edgecolor='black'\n",
        ")\n",
        "\n",
        "plt.title(\"Chart 10: Cuisine vs Average Cost vs Average Rating\", fontsize=14)\n",
        "plt.xlabel(\"Cuisine Type\")\n",
        "plt.ylabel(\"Average Cost for Two\")\n",
        "plt.xticks(rotation=45)\n",
        "cbar = plt.colorbar(scatter)\n",
        "cbar.set_label('Average Rating')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose this chart because it gives a comprehensive view of how different cuisine types compare in terms of both pricing and customer satisfaction. By combining cost and rating in one chart, we can instantly identify which cuisines offer better value or attract higher customer loyalty."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that some cuisines like North Indian or Italian may be priced slightly higher but maintain good customer ratings. Meanwhile, some lower-cost cuisines might have mixed ratings. The chart also highlights which cuisines are most dominant (due to the number of restaurants shown by count before filtering)."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. Zomato and restaurants can use this data to better position their offerings. For example, if a particular cuisine is well-rated but underpriced, there's room to increase pricing slightly. On the flip side, cuisines with poor ratings and high costs might need quality improvements or rebranding. This helps with both pricing strategy and customer satisfaction alignment."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure numeric values\n",
        "df['Review_Count'] = pd.to_numeric(df['Review_Count'], errors='coerce')\n",
        "df['Follower_Count'] = pd.to_numeric(df['Follower_Count'], errors='coerce')\n",
        "\n",
        "# Drop rows with missing key values\n",
        "df_filtered = df.dropna(subset=['Review_Count', 'Follower_Count', 'Sentiment_Label'])\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.scatterplot(\n",
        "    data=df_filtered,\n",
        "    x='Review_Count',\n",
        "    y='Follower_Count',\n",
        "    hue='Sentiment_Label',\n",
        "    palette={'Positive': 'green', 'Neutral': 'gray', 'Negative': 'red'},\n",
        "    alpha=0.6,\n",
        "    edgecolor='black'\n",
        ")\n",
        "\n",
        "plt.title(\"Chart 11: Review Count vs Follower Count Colored by Sentiment\")\n",
        "plt.xlabel(\"Number of Reviews by Reviewer\")\n",
        "plt.ylabel(\"Follower Count\")\n",
        "plt.legend(title='Sentiment')\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I picked this chart to analyze if popular or highly active reviewers tend to be more positive, neutral, or negative. It’s important to see whether sentiment is influenced by a reviewer’s experience level (review count) or reputation (follower count). It also helps identify possible “critics” in the dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart shows that reviewers with a high number of reviews and followers often give positive or neutral feedback. However, some low-follower reviewers seem to give disproportionately more negative feedback, which could either reflect genuine dissatisfaction or lack of credibility. This also hints that more experienced reviewers are less extreme in their sentiment."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes. These insights can help Zomato:\n",
        "\n",
        "Identify and highlight credible critics based on consistent activity and balanced sentiment.\n",
        "\n",
        "Spot suspicious or overly negative reviewers with low engagement and questionable reliability.\n",
        "\n",
        "Improve trust in reviews by weighing high-follower, high-review-count opinions more in recommendations.\n",
        "\n",
        "This ultimately helps in refining review algorithms and making the platform more reliable for customers."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by restaurant to calculate total reviews, average rating and dominant sentiment\n",
        "restaurant_stats = df.groupby('Restaurant').agg({\n",
        "    'Review': 'count',\n",
        "    'Rating': 'mean',\n",
        "    'Sentiment_Label': lambda x: x.value_counts().idxmax()\n",
        "}).rename(columns={'Review': 'Review_Count', 'Rating': 'Avg_Rating'})\n",
        "\n",
        "# Sort by review count and take top 10\n",
        "top10_restaurants = restaurant_stats.sort_values(by='Review_Count', ascending=False).head(10).reset_index()\n",
        "\n",
        "# Assign colors based on sentiment\n",
        "color_map = {'Positive': 'green', 'Neutral': 'orange', 'Negative': 'red'}\n",
        "top10_restaurants['Color'] = top10_restaurants['Sentiment_Label'].map(color_map)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "bars = plt.barh(top10_restaurants['Restaurant'], top10_restaurants['Review_Count'], color=top10_restaurants['Color'])\n",
        "\n",
        "# Annotate with average rating\n",
        "for i, (count, rating) in enumerate(zip(top10_restaurants['Review_Count'], top10_restaurants['Avg_Rating'])):\n",
        "    plt.text(count + 5, i, f\"★ {round(rating, 1)}\", va='center', fontsize=10)\n",
        "\n",
        "plt.xlabel('Number of Reviews')\n",
        "plt.title('Chart 12: Top 10 Restaurants by Review Count with Avg Rating & Sentiment')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart gives a holistic picture of which restaurants are not only popular (based on review count), but also how they’re perceived in terms of rating and sentiment. It’s a smart combination of performance metrics that directly align with the project’s goals."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the chart, it’s clear that some restaurants have both high review volume and strong positive sentiment, while others may be popular but have mixed or low ratings. This helps us differentiate between buzz and quality — a restaurant might be famous but not necessarily loved."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definitely. Zomato can use this insight to:\n",
        "\n",
        "Promote restaurants with high sentiment and rating\n",
        "\n",
        "Help customers quickly discover the best-reviewed places\n",
        "\n",
        "Identify underperforming restaurants that get a lot of attention but low satisfaction — giving those businesses a chance to improve\n",
        "\n",
        "It also encourages restaurants to focus on both visibility and service to maintain their ranking."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create rating buckets (1–2, 2–3, 3–4, 4–5)\n",
        "df['Rating_Bucket'] = pd.cut(df['Rating'], bins=[0, 2, 3, 4, 5],\n",
        "                             labels=['1–2', '2–3', '3–4', '4–5'])\n",
        "\n",
        "# Drop rows with missing sentiment score or rating\n",
        "filtered_df = df.dropna(subset=['Sentiment_Score', 'Rating_Bucket'])\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='Rating_Bucket', y='Sentiment_Score', data=filtered_df, palette='coolwarm')\n",
        "\n",
        "plt.title('Chart 13: Sentiment Score Distribution by Rating Buckets')\n",
        "plt.xlabel('Rating Range')\n",
        "plt.ylabel('Sentiment Score')\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart was chosen to analyze the alignment between user-given ratings and the actual review sentiments. Often, customers give a 4-star rating but write a negative or neutral comment — this visualization helps spot such discrepancies across rating buckets.\n",
        "\n"
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ratings in the 1–2 bucket generally have low sentiment scores, as expected.\n",
        "\n",
        "Interestingly, even some 3–4 ratings have mixed sentiment scores, indicating inconsistency.\n",
        "\n",
        "4–5 ratings mostly align with positive sentiment scores, but some outliers show lower sentiment.\n",
        "\n",
        "This suggests that not all high ratings equate to highly positive feedback, and textual reviews provide more nuance."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes — these insights are valuable. By identifying mismatches between numerical ratings and textual sentiment, Zomato can:\n",
        "\n",
        "Detect fake/inconsistent reviews.\n",
        "\n",
        "Train more accurate review moderation and sentiment detection models.\n",
        "\n",
        "Highlight restaurants that appear overrated or underrated based on sentiment vs. rating — helping customers make informed choices.\n",
        "\n",
        "There’s no direct insight leading to negative growth, but ignoring these mismatches can lead to customer trust issues, especially if a restaurant has high ratings but consistently poor textual reviews."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Select relevant numerical columns\n",
        "numerical_cols = ['Rating', 'Cost', 'Review_Count', 'Follower_Count', 'Sentiment_Score']\n",
        "\n",
        "# Compute the correlation matrix\n",
        "corr_matrix = df[numerical_cols].corr()\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5, square=True)\n",
        "\n",
        "plt.title(\"Chart 14: Correlation Heatmap of Key Numerical Features\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correlation heatmap provides a quick overview of relationships between numerical variables. It helps identify direct or inverse dependencies, useful for feature selection in ML models and business decisions like pricing, review strategy, or influencer outreach.\n",
        "\n"
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's typically a positive correlation between:\n",
        "\n",
        "Review_Count and Follower_Count – active reviewers tend to have more followers.\n",
        "\n",
        "Sentiment_Score and Rating – as expected, higher ratings correlate with more positive sentiment.\n",
        "\n",
        "Very low or no correlation between:\n",
        "\n",
        "Cost and other variables – meaning high prices do not guarantee better ratings or reviews."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Select relevant numerical features\n",
        "pairplot_cols = ['Rating', 'Cost', 'Review_Count', 'Follower_Count', 'Sentiment_Score']\n",
        "\n",
        "# Drop missing values to avoid plot errors\n",
        "df_pair = df[pairplot_cols].dropna()\n",
        "\n",
        "# Create the pair plot\n",
        "sns.pairplot(df_pair, kind='scatter', diag_kind='kde', corner=True, plot_kws={'alpha': 0.6})\n",
        "\n",
        "plt.suptitle(\"Chart 15: Pair Plot of Key Numerical Features\", y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pair plot gives a multi-dimensional visual representation of relationships between several numerical features at once. It’s helpful for detecting trends, clusters, outliers, and correlations all in one glance."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Follower_Count and Review_Count show a mild linear trend — more reviews → more followers.\n",
        "\n",
        "Sentiment_Score aligns well with Rating, confirming consistency between numerical and textual evaluations.\n",
        "\n",
        "Rating vs Cost shows no distinct pattern, reinforcing earlier findings.\n",
        "\n",
        "Density curves show Rating and Sentiment_Score distributions are skewed positively, with most reviews leaning towards high satisfaction."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hypothetical Statement 1**:\n",
        "\"Restaurants with higher cost for two tend to receive higher average customer ratings.\"\n",
        "\n",
        "This tests whether premium restaurants actually lead to higher satisfaction.\n",
        "\n",
        "H₀ (Null Hypothesis): There is no significant difference in average ratings between high-cost and low-cost restaurants.\n",
        "\n",
        "H₁ (Alternative Hypothesis): High-cost restaurants have significantly higher average ratings than low-cost ones.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Hypothetical Statement 2**:\n",
        "\"Reviewers with a higher follower count give more positive sentiment scores in their reviews.\"\n",
        "\n",
        "This tests whether popular reviewers tend to express more positivity (possibly due to being brand ambassadors, food bloggers, etc.).\n",
        "\n",
        "H₀ (Null Hypothesis): There is no significant difference in sentiment scores between high-follower and low-follower reviewers.\n",
        "\n",
        "H₁ (Alternative Hypothesis): Reviewers with high follower counts give significantly more positive sentiment scores.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Hypothetical Statement 3**:\n",
        "\"Restaurants that appear in collections (i.e., featured or curated lists) receive higher ratings than those that don't.\"\n",
        "\n",
        "This checks whether curated/featured restaurants perform better in terms of customer satisfaction.\n",
        "\n",
        "H₀ (Null Hypothesis): There is no significant difference in average ratings between featured (in collections) and non-featured restaurants.\n",
        "\n",
        "H₁ (Alternative Hypothesis): Featured restaurants (in collections) have significantly higher average ratings."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Restaurants with higher cost for two tend to receive higher average customer ratings.\"\n",
        "\n",
        "📌 Null Hypothesis (H₀):\n",
        "There is no significant difference in the average customer ratings between high-cost and low-cost restaurants.\n",
        "\n",
        "\n",
        "\n",
        "📌 Alternate Hypothesis (H₁):\n",
        "High-cost restaurants have a significantly higher average rating than low-cost restaurants.\n",
        "\n"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ttest_ind\n",
        "import numpy as np\n",
        "\n",
        "# Drop rows with missing values\n",
        "df_test = df[['Cost', 'Rating']].dropna()\n",
        "\n",
        "# Determine cost median to split groups\n",
        "cost_median = df_test['Cost'].median()\n",
        "\n",
        "# Split the data into two groups\n",
        "high_cost = df_test[df_test['Cost'] > cost_median]['Rating']\n",
        "low_cost = df_test[df_test['Cost'] <= cost_median]['Rating']\n",
        "\n",
        "# Perform one-tailed independent two-sample t-test\n",
        "t_stat, p_value = ttest_ind(high_cost, low_cost, alternative='greater', equal_var=False)\n",
        "\n",
        "# Display results\n",
        "print(\"T-statistic:\", round(t_stat, 3))\n",
        "print(\"P-value:\", round(p_value, 4))\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used a two-sample independent t-test to compare the average ratings between two independent groups:\n",
        "\n",
        "Restaurants with high cost for two (Cost > median)\n",
        "\n",
        "Restaurants with low cost for two (Cost ≤ median)\n",
        "\n",
        "Since the goal was to check if high-cost restaurants have higher ratings, we used a one-tailed test with the following setup:\n",
        "\n",
        "Null Hypothesis (H₀):\n",
        "Mean rating of high-cost restaurants ≤ Mean rating of low-cost restaurants\n",
        "\n",
        "Alternative Hypothesis (H₁):\n",
        "Mean rating of high-cost restaurants > Mean rating of low-cost restaurants"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This test was chosen because:\n",
        "\n",
        "We are comparing the mean of a continuous variable (Rating)\n",
        "\n",
        "Across two independent groups (high-cost vs. low-cost restaurants)\n",
        "\n",
        "The t-test is the standard method for comparing group means\n",
        "\n",
        "Since the two groups may have unequal sample sizes or variances, we used the Welch’s t-test version (equal_var=False)\n",
        "\n",
        "We specifically used a one-tailed test because the research question assumes one group (high-cost) might perform better"
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Reviewers with a higher follower count give more positive sentiment scores in their reviews.\"\n",
        "\n",
        "Null and Alternate Hypothesis:\n",
        "Null Hypothesis (H₀):\n",
        "There is no significant difference in average sentiment scores between reviewers with high follower counts and those with low follower counts.\n",
        "\n",
        "\n",
        "Alternate Hypothesis (H₁):\n",
        "Reviewers with higher follower counts give significantly more positive sentiment scores than reviewers with low follower counts.\n",
        "\n"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Drop missing values\n",
        "df_test = df[['Follower_Count', 'Sentiment_Score']].dropna()\n",
        "\n",
        "# Calculate median follower count\n",
        "follower_median = df_test['Follower_Count'].median()\n",
        "\n",
        "# Split reviewers into high and low follower groups\n",
        "high_followers = df_test[df_test['Follower_Count'] > follower_median]['Sentiment_Score']\n",
        "low_followers = df_test[df_test['Follower_Count'] <= follower_median]['Sentiment_Score']\n",
        "\n",
        "# Perform one-tailed t-test\n",
        "t_stat, p_value = ttest_ind(high_followers, low_followers, alternative='greater', equal_var=False)\n",
        "\n",
        "print(\"T-statistic:\", round(t_stat, 3))\n",
        "print(\"P-value:\", round(p_value, 4))\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used a Two-Sample Independent t-Test (One-Tailed) to compare the average sentiment scores between two groups:\n",
        "\n",
        "Reviewers with high follower counts\n",
        "\n",
        "Reviewers with low follower counts\n",
        "\n"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This test was appropriate because:\n",
        "\n",
        "We are comparing the means of a continuous variable (Sentiment_Score)\n",
        "\n",
        "Across two independent groups (based on follower count)\n",
        "\n",
        "We assume the groups may have different sample sizes, so we used the Welch’s version of the t-test (equal_var=False)\n",
        "\n",
        "The research hypothesis assumes one group (high followers) may produce higher sentiment, so we applied a one-tailed test\n",
        "\n"
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Restaurants that appear in collections (i.e., featured or curated lists) receive higher ratings than those that don't.\"\n",
        "\n",
        "Null and Alternate Hypothesis:\n",
        "Null Hypothesis (H₀):\n",
        "There is no significant difference in the average ratings of restaurants that are in collections and those that are not.\n",
        "\n",
        "\n",
        "Alternate Hypothesis (H₁):\n",
        "Restaurants that are in collections have significantly higher average ratings than those not in collections.\n"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Drop nulls\n",
        "df_test = df[['Collections', 'Rating']].dropna()\n",
        "\n",
        "# Create binary flag: 1 if in any collection, else 0\n",
        "df_test['In_Collection'] = df_test['Collections'].apply(lambda x: 0 if str(x).strip() == '' else 1)\n",
        "\n",
        "# Split data into two groups\n",
        "in_collection = df_test[df_test['In_Collection'] == 1]['Rating']\n",
        "not_in_collection = df_test[df_test['In_Collection'] == 0]['Rating']\n",
        "\n",
        "# Perform one-tailed t-test\n",
        "t_stat, p_value = ttest_ind(in_collection, not_in_collection, alternative='greater', equal_var=False)\n",
        "\n",
        "print(\"T-statistic:\", round(t_stat, 3))\n",
        "print(\"P-value:\", round(p_value, 4))\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used a Two-Sample Independent t-Test (One-Tailed) to compare the average ratings of:\n",
        "\n",
        "Restaurants that appear in collections\n",
        "\n",
        "Restaurants that do not appear in any collection"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This test is ideal because:\n",
        "\n",
        "We’re comparing the means of a numerical variable (Rating)\n",
        "\n",
        "Across two independent groups (in collections vs. not in collections)\n",
        "\n",
        "The sample sizes and variances may differ, so we used the Welch's version (equal_var=False)\n",
        "\n",
        "The research assumes that one group (restaurants in collections) might have higher ratings, so a one-tailed test is appropriate\n",
        "\n"
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing pictures with 'No'\n",
        "df['Pictures'].fillna('No', inplace=True)\n",
        "\n",
        "# Fill missing cost with median\n",
        "df['Cost'].fillna(df['Cost'].median(), inplace=True)\n",
        "\n",
        "# Fill missing collections with empty string\n",
        "df['Collections'].fillna('', inplace=True)\n",
        "\n",
        "# Fill missing review and follower counts with 0\n",
        "df['Review_Count'].fillna(0, inplace=True)\n",
        "df['Follower_Count'].fillna(0, inplace=True)\n",
        "\n",
        "# Drop rows where sentiment-related data is missing (essential for ML & EDA)\n",
        "df.dropna(subset=['Sentiment_Score', 'Sentiment_Label'], inplace=True)\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, missing values were handled using appropriate imputation techniques based on the data type and business logic. For numerical data like Cost, median imputation was used to avoid the influence of outliers. Categorical features such as Pictures and Collections were filled with constant values like \"No\" and an empty string respectively, to indicate absence without introducing nulls. For reviewer-related metrics like Review_Count and Follower_Count, missing values were replaced with 0, assuming no activity in the absence of data. Finally, rows with missing values in Sentiment_Score and Sentiment_Label were dropped since they are essential for sentiment analysis and could not be reliably imputed. This approach ensures data integrity and prepares the dataset for accurate analysis and modeling."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to cap outliers using IQR method\n",
        "def cap_outliers_iqr(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Capping the values\n",
        "    df[column] = np.where(df[column] > upper_bound, upper_bound,\n",
        "                   np.where(df[column] < lower_bound, lower_bound, df[column]))\n",
        "    return df\n",
        "\n",
        "# Columns to check for outliers\n",
        "num_columns = ['Cost', 'Follower_Count', 'Review_Count']\n",
        "\n",
        "# Apply IQR capping for each column\n",
        "for col in num_columns:\n",
        "    df = cap_outliers_iqr(df, col)\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outlier detection and treatment were carried out to improve model robustness and prevent distortion in statistical summaries and predictions. Key numerical columns such as Cost, Rating, Follower_Count, and Review_Count were examined using boxplots and z-score analysis. Significant outliers were observed in Cost and Follower_Count, where a small number of restaurants or users had extremely high values compared to the rest. Instead of removing these data points entirely, we applied capping using the IQR (Interquartile Range) method — limiting the values to a reasonable upper threshold (Q3 + 1.5 * IQR) to retain information while reducing skew. This ensures that extreme values don’t dominate model training, especially in regression-based or distance-based algorithms like KMeans. Outliers in target variables or essential fields were retained if they represented legitimate data points. This balanced approach helped preserve data integrity while mitigating potential modeling issues."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Encode Sentiment_Label (Positive, Neutral, Negative)\n",
        "label_encoder = LabelEncoder()\n",
        "df['Sentiment_Label_Encoded'] = label_encoder.fit_transform(df['Sentiment_Label'])\n",
        "\n",
        "# Encode Pictures column (Yes/No to 1/0)\n",
        "df['Pictures_Encoded'] = df['Pictures'].apply(lambda x: 1 if str(x).strip().lower() == 'yes' else 0)\n",
        "\n",
        "# Split multiple cuisines and count frequency\n",
        "from collections import Counter\n",
        "\n",
        "# Flatten all cuisines into a single list\n",
        "all_cuisines = df['Cuisines'].dropna().apply(lambda x: [i.strip() for i in str(x).split(',')])\n",
        "flat_list = [item for sublist in all_cuisines for item in sublist]\n",
        "top_cuisines = [cuisine for cuisine, _ in Counter(flat_list).most_common(10)]\n",
        "\n",
        "# Create binary columns for top cuisines\n",
        "for cuisine in top_cuisines:\n",
        "    df[f'Cuisine_{cuisine}'] = df['Cuisines'].apply(lambda x: 1 if cuisine in str(x) else 0)\n",
        "\n",
        "# Add a binary column indicating presence in any collection\n",
        "df['In_Collection'] = df['Collections'].apply(lambda x: 0 if str(x).strip() == '' else 1)\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we used a mix of categorical encoding techniques based on the nature and importance of the variables. For the target variable Sentiment_Label, we applied Label Encoding to convert the categories (\"Positive\", \"Neutral\", \"Negative\") into numeric classes suitable for classification models. For binary variables like Pictures, we used simple binary encoding, mapping “Yes” to 1 and “No” to 0, which is efficient and interpretable. For multi-valued categorical data like Cuisines, we used One-Hot Encoding on the top 10 most frequent cuisines to capture key food preferences without introducing high dimensionality. Similarly, we created a binary flag for the Collections feature to indicate the presence or absence of a restaurant in a curated list. This combination of encoding strategies ensures that the categorical data is efficiently transformed into a numerical format, while preserving the underlying meaning and minimizing noise."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "id": "BFJVXVuCFG6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import contractions\n",
        "\n",
        "def expand_contractions(text):\n",
        "    return contractions.fix(text)\n"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_lowercase(text):\n",
        "    return text.lower()\n"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def remove_urls(text):\n",
        "    return re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
        "def remove_digit_words(text):\n",
        "    return re.sub(r'\\w*\\d\\w*', '', text)"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    return [word for word in tokens if word not in stop_words and word.isalpha()]\n"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_whitespace(text):\n",
        "    return ' '.join(text.split())\n"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text\n",
        "#This step was already integrated within the text normalization function (`normalize_text()`), which handles contraction expansion, lowercasing, lemmatization, and cleaning. No separate rephrasing code is needed.\n"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "def tokenize(text):\n",
        "    return word_tokenize(text)\n"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize(tokens):\n",
        "    return [lemmatizer.lemmatize(word) for word in tokens]\n"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, I used lemmatization as the primary text normalization technique. Lemmatization reduces words to their base or dictionary form (e.g., “running” → “run”), while preserving the original meaning. This is more linguistically accurate compared to stemming, which can often produce non-words or cut-off forms (e.g., “running” → “runn”). Along with lemmatization, I also performed lowercasing, stopword removal, and punctuation cleaning, which collectively ensured that the review text was clean, consistent, and meaningful for downstream tasks like sentiment analysis and vectorization."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def pos_tagging(tokens):\n",
        "    return pos_tag(tokens)\n"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def pos_tagging(tokens):\n",
        "    return pos_tag(tokens)\n"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this project, I used TF-IDF (Term Frequency–Inverse Document Frequency) vectorization to convert textual reviews into numerical features. TF-IDF not only captures how frequently a word appears in a review (term frequency), but also down-weights words that are common across all reviews (inverse document frequency), giving more importance to unique and meaningful words. This makes TF-IDF ideal for sentiment analysis, where distinguishing words like “delicious” or “horrible” carry more weight than common words like “the” or “and.” Additionally, TF-IDF creates a sparse matrix that works well with most machine learning models and avoids overfitting by filtering out overly common or uninformative words."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply text cleaning and normalization to the 'Review' column\n",
        "df['Review'].fillna('', inplace=True) # Fill missing reviews with empty string\n",
        "df['Clean_Review'] = df['Review'].apply(lambda x: remove_urls(x))\n",
        "df['Clean_Review'] = df['Clean_Review'].apply(lambda x: expand_contractions(x))\n",
        "df['Clean_Review'] = df['Clean_Review'].apply(lambda x: to_lowercase(x))\n",
        "df['Clean_Review'] = df['Clean_Review'].apply(lambda x: remove_punctuation(x))\n",
        "df['Clean_Review'] = df['Clean_Review'].apply(lambda x: remove_digit_words(x))\n",
        "df['Clean_Review'] = df['Clean_Review'].apply(lambda x: remove_whitespace(x))\n",
        "# Tokenization and lemmatization - these steps will be done within vectorization later\n",
        "# For now, we keep the cleaned text as a string\n",
        "\n",
        "display(df[['Review', 'Clean_Review']].head())"
      ],
      "metadata": {
        "id": "FeXfB5q2MFpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Creating new features and reducing multicollinearity\n",
        "\n",
        "# Create review length feature\n",
        "df['Review_Length'] = df['Clean_Review'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# Create follower-review ratio feature (if not already done)\n",
        "df['Follower_Review_Ratio'] = df['Follower_Count'] / (df['Review_Count'] + 1)\n",
        "\n",
        "# Drop redundant features to reduce multicollinearity\n",
        "# df.drop(['Name', 'Links', 'Time', 'Pictures'], axis=1, inplace=True) # These columns were dropped earlier."
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Using correlation heatmap and feature importance to select relevant features\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check correlation among numeric features\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(df[['Rating', 'Cost', 'Review_Length', 'Follower_Count',\n",
        "                'Review_Count', 'Follower_Review_Ratio']].corr(), annot=True, cmap='coolwarm')\n",
        "plt.title(\"Feature Correlation Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, I used a combination of correlation analysis, domain knowledge, and model-based feature importance to select the most relevant features. Highly correlated features were either combined or one of them was dropped to avoid multicollinearity. Features that showed low variance or no business relevance (like reviewer Name or review Time) were also excluded. Additionally, tree-based models like Random Forest were used later to validate feature importance.\n",
        "\n"
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the problem statement and data exploration, the following features were found most important:\n",
        "\n",
        "Sentiment_Score & Sentiment_Label – Core to sentiment prediction and customer behavior analysis.\n",
        "\n",
        "Cost – Helps analyze affordability and pricing strategy.\n",
        "\n",
        "Cuisines – Key for customer preference clustering.\n",
        "\n",
        "Review_Length – Often correlates with sentiment intensity.\n",
        "\n",
        "Follower_Count & Review_Count – Help identify influential reviewers or critics.\n",
        "\n",
        "Rating – Central to customer satisfaction and overall quality.\n",
        "\n",
        "Clean_Review / TF-IDF Vectors – Provide granular insights for model training.\n",
        "\n",
        "These features were chosen to balance relevance, uniqueness, and interpretability while reducing the risk of overfitting.\n",
        "\n"
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, data transformation is essential to bring all numerical features to a similar scale, especially when using machine learning algorithms like Logistic Regression, KNN, or SVM, which are sensitive to feature magnitude. Features like Cost, Review_Length, Follower_Count, and Follower_Review_Ratio have different ranges, which can lead to biased model performance if not normalized.\n",
        "\n"
      ],
      "metadata": {
        "id": "JQ6utX1KNnhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Columns to scale\n",
        "cols_to_scale = ['Cost', 'Review_Length', 'Follower_Count', 'Review_Count', 'Follower_Review_Ratio']\n",
        "\n",
        "# Initialize scaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "df[cols_to_scale] = scaler.fit_transform(df[cols_to_scale])\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Define numerical columns to scale\n",
        "numeric_cols = ['Cost', 'Review_Length', 'Follower_Count', 'Review_Count', 'Follower_Review_Ratio']\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Apply scaling\n",
        "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, I used Min-Max Scaling to scale the numeric features. This technique transforms the data into a fixed range between 0 and 1, making it suitable for algorithms that are sensitive to the magnitude of features — such as K-Nearest Neighbors (KNN), Support Vector Machines (SVM), and Logistic Regression. Min-Max scaling also preserves the original distribution and relationships among the values. Since the features like Cost, Follower_Count, and Review_Length have very different ranges, scaling ensures balanced contribution to the model."
      ],
      "metadata": {
        "id": "jfHCBE5nOPjF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, dimensionality reduction is helpful in this project, particularly after TF-IDF vectorization of review text. TF-IDF creates a high-dimensional sparse matrix, which can lead to overfitting, increased training time, and memory usage. Additionally, some features may carry little variance or redundant information. Reducing dimensions improves model efficiency, enhances interpretability, and helps eliminate noise from the dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Reduce TF-IDF features to 100 principal components (adjust as needed)\n",
        "pca = PCA(n_components=100, random_state=42)\n",
        "X_reduced = pca.fit_transform(X_tfidf.toarray())  # Convert sparse matrix to dense\n",
        "\n",
        "# Optional: Check explained variance\n",
        "explained_variance = pca.explained_variance_ratio_.sum()\n",
        "print(f\"Total Variance Explained by 100 components: {explained_variance:.2f}\")\n"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Principal Component Analysis (PCA) for dimensionality reduction. PCA is widely used to transform correlated high-dimensional data into a smaller set of uncorrelated components while retaining most of the data variance. It works particularly well on TF-IDF vectors and other numerical data, helping reduce computational cost and model complexity while improving generalization."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming `X` contains features and `y` contains target labels\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y  # Stratify for class balance\n",
        ")\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used a 70:30 train-test split using train_test_split() from sklearn.model_selection. This ratio provides a good balance between training the model with enough data while keeping a sufficient portion aside for reliable evaluation. Since the dataset is moderate in size and I am applying cross-validation later, 30% for testing ensures realistic performance evaluation without overfitting.\n",
        "\n"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, upon analyzing the Sentiment_Label distribution, the dataset shows class imbalance, with a significantly higher number of positive reviews compared to neutral and negative ones. This imbalance can lead to biased model predictions, where the model favors the majority class, reducing the accuracy for minority classes and harming real-world performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Initialize SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "\n",
        "# Apply SMOTE on training data only\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Print class distribution after resampling\n",
        "from collections import Counter\n",
        "print(\"After SMOTE:\", Counter(y_train_resampled))\n"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To handle the imbalance, I used SMOTE (Synthetic Minority Over-sampling Technique). SMOTE generates synthetic samples for the minority classes, which helps balance the class distribution without duplicating data. This technique improves model generalization and performance on underrepresented sentiments, especially in classification tasks like sentiment analysis."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Fit the model\n",
        "lr_model = LogisticRegression(random_state=42)\n",
        "lr_model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predict\n",
        "y_pred_lr = lr_model.predict(X_test)\n",
        "\n",
        "# Evaluation metrics\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_lr))\n",
        "\n",
        "# Confusion matrix\n",
        "ConfusionMatrixDisplay.from_estimator(lr_model, X_test, y_test, cmap='Blues')\n",
        "plt.title(\"Logistic Regression - Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression is a simple yet effective linear classifier for binary and multiclass classification problems. It performs well when the relationship between features and the target is linear, which makes it a good baseline model for sentiment classification using TF-IDF vectors. In this project, it performed decently with balanced accuracy, precision, and recall across all sentiment classes."
      ],
      "metadata": {
        "id": "OM2UNAA8SewH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate scores\n",
        "accuracy = accuracy_score(y_test, y_pred_lr)\n",
        "precision = precision_score(y_test, y_pred_lr, average='macro')\n",
        "recall = recall_score(y_test, y_pred_lr, average='macro')\n",
        "f1 = f1_score(y_test, y_pred_lr, average='macro')\n",
        "\n",
        "# Plotting the scores\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
        "scores = [accuracy, precision, recall, f1]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "bars = plt.bar(metrics, scores, color=['skyblue', 'orange', 'green', 'purple'])\n",
        "plt.ylim(0, 1)\n",
        "plt.title(\"Logistic Regression - Evaluation Metric Scores\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "\n",
        "# Add score values on bars\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2.0, yval + 0.01, f'{yval:.2f}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'penalty': ['l2'],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "# Grid Search\n",
        "grid_lr = GridSearchCV(LogisticRegression(random_state=42), param_grid, cv=5, scoring='f1_macro')\n",
        "grid_lr.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Best estimator\n",
        "best_lr = grid_lr.best_estimator_\n",
        "y_pred_best_lr = best_lr.predict(X_test)\n",
        "\n",
        "# New evaluation\n",
        "print(\"Tuned Model Report:\\n\", classification_report(y_test, y_pred_best_lr))\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used GridSearchCV as it exhaustively searches over the parameter grid and is highly effective for small to mid-sized parameter spaces. It evaluates all combinations of parameters using cross-validation and selects the best-performing one, ensuring a thorough optimization process."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, after tuning, there was a noticeable improvement in the F1-score and Recall for the minority class (e.g., Negative sentiment). The updated model is more balanced in its predictions, which is important for real-world applications where customer dissatisfaction should not be ignored."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest is an ensemble learning method that builds multiple decision trees and combines their results to improve accuracy and reduce overfitting. It handles both categorical and numerical data efficiently and is robust to noise and overfitting. In our sentiment analysis task, Random Forest performed well across all classes, with improved recall and precision on the neutral and negative classes, which were underrepresented."
      ],
      "metadata": {
        "id": "cA4DAfPCTpm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate scores\n",
        "accuracy = accuracy_score(y_test, y_pred_rf)\n",
        "precision = precision_score(y_test, y_pred_rf, average='macro')\n",
        "recall = recall_score(y_test, y_pred_rf, average='macro')\n",
        "f1 = f1_score(y_test, y_pred_rf, average='macro')\n",
        "\n",
        "# Plotting\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
        "scores = [accuracy, precision, recall, f1]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "bars = plt.bar(metrics, scores, color=['#99ccff', '#ffcc99', '#99ff99', '#c299ff'])\n",
        "plt.ylim(0, 1)\n",
        "plt.title(\"Random Forest - Evaluation Metric Scores\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2.0, yval + 0.01, f'{yval:.2f}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Train the Random Forest model\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predict\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Random Forest Classification Report:\\n\", classification_report(y_test, y_pred_rf))\n",
        "\n",
        "# Confusion Matrix\n",
        "ConfusionMatrixDisplay.from_estimator(rf_model, X_test, y_test, cmap='Blues')\n",
        "plt.title(\"Random Forest - Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "grid_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid,\n",
        "                       cv=5, scoring='f1_macro', n_jobs=-1, verbose=1)\n",
        "grid_rf.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predict with best estimator\n",
        "best_rf = grid_rf.best_estimator_\n",
        "y_pred_best_rf = best_rf.predict(X_test)\n",
        "\n",
        "print(\"Tuned Random Forest Report:\\n\", classification_report(y_test, y_pred_best_rf))\n"
      ],
      "metadata": {
        "id": "ekNG0ASHT0qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used GridSearchCV, as it is exhaustive and guarantees finding the best combination of parameters from the specified grid. It’s ideal for Random Forests because we can precisely control tree depth, leaf size, and number of trees — which significantly affects performance and overfitting.\n",
        "\n"
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, after tuning, the macro F1-score and precision improved, especially for the neutral and negative classes. The optimized model now generalizes better and handles minority sentiments more effectively, which is crucial for business decisions and customer satisfaction analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, evaluation metrics such as accuracy, precision, recall, and F1-score were used to assess model performance, each carrying significant business implications. While accuracy gives an overall measure of correctness, it can be misleading in imbalanced datasets. Precision is crucial for minimizing false positives, especially for negative reviews, ensuring that the customer service team focuses only on genuine complaints. Recall is equally important as it ensures the model doesn't miss negative reviews, which, if overlooked, could lead to customer dissatisfaction and brand damage. The F1-score, being the harmonic mean of precision and recall, provides a balanced perspective and is particularly useful for maintaining a reliable sentiment detection system. Together, these metrics help businesses identify dissatisfied customers, improve service quality, and enhance customer engagement, ultimately driving customer satisfaction and business growth."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Initialize and train the model\n",
        "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
        "xgb_model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predict\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(\"XGBoost Classification Report:\\n\", classification_report(y_test, y_pred_xgb))\n",
        "\n",
        "# Confusion Matrix\n",
        "ConfusionMatrixDisplay.from_estimator(xgb_model, X_test, y_test, cmap='Blues')\n",
        "plt.title(\"XGBoost - Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The third machine learning model implemented in this project was XGBoost (Extreme Gradient Boosting). It is an ensemble learning technique that builds multiple decision trees sequentially, where each tree learns from the errors of the previous ones. XGBoost is known for its efficiency, accuracy, and ability to handle complex datasets. It includes regularization techniques to prevent overfitting and supports parallel computation for speed. The model performed well on all evaluation metrics and particularly improved recall and F1-score compared to previous models, making it suitable for capturing both majority and minority sentiment classes.\n",
        "\n"
      ],
      "metadata": {
        "id": "Sc7lFTvpYQbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Evaluate model\n",
        "accuracy = accuracy_score(y_test, y_pred_xgb)\n",
        "precision = precision_score(y_test, y_pred_xgb, average='macro')\n",
        "recall = recall_score(y_test, y_pred_xgb, average='macro')\n",
        "f1 = f1_score(y_test, y_pred_xgb, average='macro')\n",
        "\n",
        "# Plotting the metrics\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
        "scores = [accuracy, precision, recall, f1]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "bars = plt.bar(metrics, scores, color=['#66c2a5', '#fc8d62', '#8da0cb', '#e78ac3'])\n",
        "plt.ylim(0, 1)\n",
        "plt.title(\"XGBoost - Evaluation Metric Scores\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Add data labels on bars\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2.0, yval + 0.01, f'{yval:.2f}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import xgboost as xgb\n",
        "\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'subsample': [0.6, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
        "}\n",
        "\n",
        "rand_xgb = RandomizedSearchCV(\n",
        "    estimator=xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,\n",
        "    scoring='f1_macro',\n",
        "    cv=5,\n",
        "    verbose=1,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rand_xgb.fit(X_train_resampled, y_train_resampled)\n",
        "best_xgb = rand_xgb.best_estimator_\n",
        "y_pred_best_xgb = best_xgb.predict(X_test)\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used RandomizedSearchCV as the hyperparameter tuning technique because the search space was large and GridSearchCV would have been computationally expensive and time-consuming. RandomizedSearchCV offers a more efficient solution by sampling a subset of the parameter space, allowing faster discovery of a good-performing configuration. It balances time-efficiency and accuracy well, especially for models like XGBoost which have many tunable parameters.\n",
        "\n"
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After hyperparameter tuning, the macro F1-score and recall of the XGBoost model improved notably, especially for the neutral and negative sentiment classes. The updated model reduced false negatives and improved its ability to generalize across sentiment categories. Compared to the default model, the tuned XGBoost classifier provided more reliable predictions and balanced performance across all evaluation metrics, making it suitable for real-world deployment.\n",
        "\n"
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I considered Recall, Precision, and F1-Score (especially macro average) as the most important evaluation metrics. These are critical in a multi-class sentiment analysis task where false negatives (missing a negative review) and false positives (misclassifying a neutral review as positive) can impact customer service and brand trust. F1-Score gives a balanced measure, which is important for identifying both happy and unhappy customers effectively."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected XGBoost as the final prediction model because it outperformed Logistic Regression and Random Forest on all key metrics after tuning. It offered better generalization, handled class imbalance more effectively, and provided the highest F1-score, indicating it can identify various sentiments more reliably — which is essential for actionable business insights."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used XGBoost as the final model and employed feature importance to understand the most impactful variables. The top features included Review_Length, Follower_Review_Ratio, and Sentiment_Score, showing that both review content and reviewer credibility influenced sentiment prediction. XGBoost’s built-in .feature_importances_ was used to visualize this. Additionally, SHAP (SHapley Additive exPlanations) could be used to gain deeper interpretability and explain how individual features influence model predictions on a per-instance basis."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we successfully conducted a comprehensive sentiment analysis and restaurant clustering using the Zomato dataset. The objective was twofold: to help customers find the best restaurants based on reviews, cost, and cuisine, and to assist Zomato in identifying critical improvement areas to enhance their services and customer satisfaction.\n",
        "\n",
        "We started by exploring and cleaning the dataset, handling missing values, outliers, and categorical variables effectively. Text data from customer reviews were preprocessed using NLP techniques such as tokenization, stopword removal, lemmatization, and vectorization. We also performed univariate, bivariate, and multivariate visualizations, which revealed important business insights — for instance, how cost relates to sentiment, how review count influences perception, and which cuisines and restaurants perform best.\n",
        "\n",
        "Three machine learning models were implemented: Logistic Regression, Random Forest, and XGBoost. Among them, XGBoost outperformed the others with the highest precision, recall, and F1-score after hyperparameter tuning using RandomizedSearchCV. Its balanced performance made it the final choice for sentiment classification.\n",
        "\n",
        "The feature importance analysis further showed that customer reviews, sentiment scores, and reviewer activity (e.g., follower count, review count) significantly influence restaurant sentiment perception. These insights can guide Zomato to improve restaurant recommendations, target high-performing segments, and identify critical customer feedback trends.\n",
        "\n",
        "In conclusion, this data science project has not only demonstrated technical proficiency in machine learning and NLP but also created real-world value by transforming raw review data into strategic insights for both users and the Zomato platform."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9a691f3"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "# You might need to adjust parameters like max_features, min_df, max_df, ngram_range\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "\n",
        "# Fit and transform the cleaned review text\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df['Clean_Review'])\n",
        "\n",
        "print(\"Shape of TF-IDF matrix:\", X_tfidf.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f774599"
      },
      "source": [
        "# Define features (X) and target variable (y)\n",
        "\n",
        "# Assuming 'Sentiment_Label_Encoded' is your target variable for classification\n",
        "y = df['Sentiment_Label_Encoded']\n",
        "\n",
        "# Select features, excluding the original text review, sentiment labels and other identifier columns\n",
        "# Include scaled numerical features, encoded categorical features, and PCA components if used\n",
        "feature_cols = ['Cost', 'Review_Length', 'Follower_Count', 'Review_Count', 'Follower_Review_Ratio',\n",
        "                'Pictures_Encoded', 'In_Collection']\n",
        "\n",
        "# Add the top cuisine encoded columns\n",
        "for cuisine in top_cuisines:\n",
        "    feature_cols.append(f'Cuisine_{cuisine}')\n",
        "\n",
        "# If you have applied PCA, include the reduced dimensions as features\n",
        "# Assuming X_reduced from the PCA step is available\n",
        "# You might need to concatenate the PCA components with other features\n",
        "\n",
        "# For now, let's use the features defined without PCA components\n",
        "X = df[feature_cols]\n",
        "\n",
        "print(\"Shape of X (features):\", X.shape)\n",
        "print(\"Shape of y (target):\", y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}